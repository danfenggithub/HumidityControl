import pandas as pd
import numpy as np
import tensorflow as tf
import shutil
from .memories import VanillaMemory
from .agents import DQNAgent
from .gymRoom import *
from .utils import *
from .gymProfile import *

import subprocess
import threading


def thread1():
    # Open starccm+ server in Linux background
    subprocess.call("cd javafile && " + start_starccm_server, shell=True)


def thread2():
    # Thread 2 executes Java macro commands continuously in the background,
    # and each round of loop represents a round
    for i in range(100000):
        subprocess.call(runforStep_gymGame_command, shell=True)


def run(env, a):
    # Modify the messenger to check info Java release, execute the next macro command
    with open("javafile/info.txt", "w") as f:
        f.write("false")


    thread_thred1 = threading.Thread(target=thread1)
    thread_thred1.start()
    time.sleep(2)

    #Create a path to the generated file.
    # out/logs/ is mainly used to store log files.
    # out/csvfile/ is the record of each round.
    # out/savemodel/ is used to save model super parameters
    all_log_path = ['out/logs/', 'out/csvfile/', 'out/tensorboard', 'out/savemodel/local/', 'out/savemodel/target/']
    for pathName in all_log_path:
        if not os.path.exists(pathName):
            os.makedirs(pathName)

    # log enable
    log()

    # Delete historical files existing in the system
    checkExistFile()

    Reward_history = []
    FanPower_history = []

    # Let macro commands hang in the background
    thread_thred2 = threading.Thread(target=thread2)
    thread_thred2.start()
    eposideFlag = True
    while True:  # Run until solved
        # print(threading.active_count())
        # Delete "RHfiles.csv" generated by history
        if os.path.exists(RHfile):
            os.remove(RHfile)
        # Initialization of rewards and fan_power in a round
        rewards_history = []
        fan_power_history = []
        # Select initial action
        startAction = [50, 50, 50, 0, 0, 0]
        # Simulation environment initialization
        env.make(startAction)
        time.sleep(6)
        stepFlag = True

        while True:
            # Get the current environment
            state, _ = env.state(a.t_step)
            # Get the current action according to the environment
            action = a.act(state)
            # Modify the action and execute the simulation environment
            state_next, reward, done, fan_power = env.steprun(action, a.t_step)
            # # Recalculate if model floating-point overflow
            state_flag = np.mean(state_next)
            if state_flag > 61 or state_flag < 40:
                stepFlag = False
                break
            # Store the reward and fan_power of each step in a round
            rewards_history.append(reward)
            fan_power_history.append(fan_power)

            print("timestep", a.t_step, "action:", action, "reward:", reward, "fanpower:",
                  fan_power)
            # Store data and train models
            a.step(state, action, reward, state_next, done)

            # Judge whether to end the round
            if done is True:
                break

        # If the model diverges, this round will not be recorded
        if stepFlag is False:
            eposideFlag = False
            continue
        if eposideFlag is False:
            eposideFlag = True
            continue
        episode = a.episodes - 1

        Reward_history.append(sum(rewards_history))
        FanPower_history.append(sum(fan_power_history))

        print('episode', episode, 'score', sum(rewards_history), 'score_max', max(Reward_history), "FanPowerSUM",
              sum(fan_power_history))

        # save episode_reward_history and each_episode_rewards_history

        name = [str(episode)]

        pd.DataFrame(data=[rewards_history], index=name). \
            to_csv(each_episode_rewards_history_csv, sep=',', mode='a', encoding='utf-8')

        pd.DataFrame(data=[fan_power_history], index=name). \
            to_csv(each_FanPower_history_csv, sep=',', mode='a', encoding='utf-8')

        pd.DataFrame(data=[Reward_history], index=name). \
            to_csv(episode_reward_history_csv, sep=',', encoding='utf-8')

        pd.DataFrame(data=[FanPower_history], index=name). \
            to_csv(FanPower_history_csv, sep=',', encoding='utf-8')

        pd.DataFrame(data=np.array([a.losses]).reshape(-1, 1)). \
            to_csv(Loss_history_csv, encoding='utf-8')

        # Save RHfile.csv history of each round as RHfile_episode.csv
        dstFile = all_log_path[1] + 'RHfile_' + str(episode) + '.csv'
        try:
            shutil.copyfile(RHfile, dstFile)
        except Exception as e:
            print(e)

        # Save model super parameters
        if episode % 100 == 0:
            # a.network_local.save_weights(all_log_path[3] + 'RHfile_' + str(episode))
            a.network_target.save_weights(all_log_path[4] + 'RHfile_' + str(episode))

        # Judge the conditions for model convergence and training completion
        if len(Reward_history) >= 10 and (np.mean(Reward_history[-10:])) > 20:
            print("Solved at episode {}!".format(episode))
            break